# Feature Specification: SIGINT Unified Multi-Source Analysis Architecture

**Generated**: 2026-01-09T18:52:57.926173
**Status**: Draft
**Source**: Auto-generated by Smactorio workflow

---

## Overview

Refactor the SIGINT system into a three-stage unified architecture that separates data ingestion from analysis. Stage 1 handles independent ingestion of RSS, Twitter, Polymarket, and Ticker data to S3. Stage 2 introduces a unified analyzer Lambda that performs cross-source LLM analysis with correlation boosting. Stage 3 generates narrative explanations with source attribution and implications.

## Problem Statement

The current SIGINT architecture processes each data source independently, missing opportunities to identify correlations between sources and provide richer analysis. Twitter velocity signals that could boost confidence in related RSS items are not being leveraged, and there is no unified narrative layer to explain implications across sources.

---

## User Stories

### US1 - Multi-Source Raw Data Ingestion (Priority: P0)

As a **Data Engineer**, I want to **configure separate ingestion pipelines for RSS, Twitter, Polymarket, and Ticker data that store raw data to S3**, so that **each data source can be collected independently with its own schedule and error handling without affecting other sources**.

**Independent Test**: Trigger each ingestion Lambda independently and verify that raw data appears in the correct S3 bucket path with proper metadata, then intentionally fail one Lambda and confirm others continue operating

**Acceptance Criteria**:

1. RSS ingestion Lambda stores raw RSS data to S3 with timestamp and source metadata
2. Twitter ingestion Lambda stores raw tweet data to S3 with engagement metrics and velocity data
3. Polymarket ingestion Lambda stores raw prediction market data to S3 with odds and volume
4. Ticker ingestion Lambda stores raw financial data to S3 with price and volume information
5. Each ingestion pipeline operates independently and can fail without affecting others
6. S3 data is organized by source type, category, and timestamp

### US2 - Unified Cross-Source Analysis (Priority: P0)

As a **Intelligence Analyst**, I want to **have a single analyzer Lambda load all sources per category and run unified LLM analysis**, so that **I can see correlations between sources that would be missed when analyzing each source independently**.

**Independent Test**: Seed S3 with test data from multiple sources about the same topic, trigger the analyzer Lambda, and verify the output references correlations between the sources

**Acceptance Criteria**:

1. Analyzer Lambda loads all raw data from S3 for a given category
2. Single LLM prompt includes context from all available sources (RSS, Twitter, Polymarket, Tickers)
3. Analysis output identifies cross-source correlations and patterns
4. Analyzer runs after all ingestion pipelines complete for a given time window
5. Analysis results are stored with references to source data used

### US3 - Twitter Velocity Boosting (Priority: P1)

As a **Intelligence Analyst**, I want to **have Twitter velocity metrics automatically boost the relevance score of correlated RSS items**, so that **breaking news that is gaining traction on Twitter surfaces related RSS items more prominently**.

**Independent Test**: Create test RSS item and matching high-velocity Twitter data, run analysis, and verify the RSS item's score is higher than an identical RSS item without Twitter correlation

**Acceptance Criteria**:

1. Twitter velocity is calculated based on tweet frequency, retweet rate, and engagement acceleration
2. RSS items with semantic similarity to high-velocity Twitter topics receive boosted scores
3. Boosting factor is configurable and documented
4. Boosted items include metadata indicating the Twitter correlation that triggered the boost
5. Items without Twitter correlation maintain their baseline scores

### US4 - Source Attribution Tags (Priority: P1)

As a **End User**, I want to **see source tags [RSS] [Twitter] [Polymarket] [Ticker] on each item in the display**, so that **I can quickly identify which sources contributed to each insight and assess credibility accordingly**.

**Independent Test**: Query the API for analyzed items and verify each response includes source_tags array with valid source identifiers matching the contributing data sources

**Acceptance Criteria**:

1. Each displayed item shows one or more source tags indicating data origins
2. Tags are visually distinct and consistently styled
3. Items with multiple source correlations show all contributing source tags
4. Tags are clickable/hoverable to show source details
5. API response includes source attribution metadata

### US5 - Narrative Generation with Implications (Priority: P1)

As a **End User**, I want to **read explanatory narrative paragraphs that describe what is happening and its implications**, so that **I can quickly understand the significance of events without having to piece together raw data myself**.

**Independent Test**: Trigger narrative generation for a test analysis result and verify output contains both descriptive and implication content with source references

**Acceptance Criteria**:

1. Narrative writer Lambda generates human-readable paragraphs for each significant finding
2. Narratives explain the 'what' (event description) and 'so what' (implications)
3. Narratives reference specific sources and data points that support the analysis
4. Narrative length is appropriate (2-4 sentences for standard items, longer for major events)
5. Narratives are stored alongside analysis results and accessible via API

### US6 - Category-Based Source Loading (Priority: P2)

As a **System Administrator**, I want to **configure which sources are loaded for each analysis category**, so that **I can optimize analysis by including only relevant sources per category and reduce unnecessary processing**.

**Independent Test**: Modify category configuration to exclude a source, trigger analysis, and verify the excluded source data is not loaded or referenced in results

**Acceptance Criteria**:

1. Configuration file or environment variables define source-to-category mappings
2. Analyzer Lambda respects category configuration when loading source data
3. Categories can include any combination of available sources
4. Configuration changes do not require code deployment
5. Invalid configurations are logged and fail gracefully

---

## Entities

### RawSourceData
**Type**: entity
**Description**: Raw data ingested from any source before analysis
**Attributes**:
  - source_type
  - category
  - timestamp
  - raw_content
  - metadata
  - s3_path

### AnalysisResult
**Type**: entity
**Description**: Output from unified LLM analysis across sources
**Attributes**:
  - category
  - timestamp
  - findings
  - correlations
  - source_references
  - boost_factors

### Narrative
**Type**: entity
**Description**: Human-readable explanation generated from analysis
**Attributes**:
  - analysis_id
  - description
  - implications
  - source_citations
  - generated_at

### TwitterVelocity
**Type**: entity
**Description**: Velocity metrics for Twitter topics
**Attributes**:
  - topic
  - tweet_count
  - retweet_rate
  - engagement_acceleration
  - time_window
  - calculated_at

## Functional Requirements

FR-001: FR-001: System shall ingest RSS data independently and store raw content to S3
FR-002: FR-002: System shall ingest Twitter data independently and store raw content with engagement metrics to S3
FR-003: FR-003: System shall ingest Polymarket data independently and store raw content to S3
FR-004: FR-004: System shall ingest Ticker data independently and store raw content to S3
FR-005: FR-005: Analyzer Lambda shall load all source data from S3 for a given category
FR-006: FR-006: Analyzer Lambda shall perform single unified LLM analysis across all loaded sources
FR-007: FR-007: System shall calculate Twitter velocity based on tweet frequency and engagement
FR-008: FR-008: System shall boost RSS item scores when correlated with high-velocity Twitter topics
FR-009: FR-009: Narrative writer Lambda shall generate explanatory paragraphs from analysis results
FR-010: FR-010: Narrative writer shall include implications in generated content
FR-011: FR-011: Display layer shall show source tags [RSS] [Twitter] [Polymarket] [Ticker] on items
FR-012: FR-012: API shall include source attribution metadata in responses

## Non-Functional Requirements

NFR-001: NFR-001: Each ingestion pipeline shall operate independently without single points of failure
NFR-002: NFR-002: Raw data in S3 shall be organized by source type, category, and timestamp for efficient retrieval
NFR-003: NFR-003: Analyzer Lambda shall complete processing within Lambda timeout limits
NFR-004: NFR-004: System shall maintain audit trail of which sources contributed to each analysis
NFR-005: NFR-005: Configuration changes shall not require code deployment

## Business Rules

- BR-001: Twitter velocity boost shall only apply when semantic similarity threshold is met between Twitter topic and RSS item
- BR-002: Items without Twitter correlation shall maintain baseline scores without penalty
- BR-003: All displayed items must include at least one source tag
- BR-004: Narratives must reference specific sources that support the analysis claims

## Assumptions

- Existing Lambda infrastructure and S3 buckets are available for use
- LLM API (likely OpenAI or similar) is available and has sufficient capacity
- Categories are predefined and consistent across all source types
- Twitter API access provides sufficient data for velocity calculations
- Polymarket and Ticker data sources have existing API integrations

## Open Questions

- [ ] What is the specific formula for Twitter velocity calculation and boost factor?
- [ ] What time window should be used for loading source data per analysis run?
- [ ] How should the system handle missing sources (e.g., no Twitter data for a category)?
- [ ] What is the trigger mechanism for the analyzer Lambda - time-based or event-based?
- [ ] Should narratives be generated for all findings or only those above a significance threshold?
- [ ] What is the retention policy for raw S3 data?
- [ ] How should semantic similarity between Twitter and RSS be calculated?

## Success Criteria

- All four ingestion pipelines (RSS, Twitter, Polymarket, Ticker) independently store raw data to S3
- Analyzer Lambda successfully loads and processes multi-source data per category
- Cross-source correlations are identified and documented in analysis output
- Twitter velocity boosting demonstrably affects RSS item scores when correlation exists
- Narrative paragraphs are generated with both description and implications
- All displayed items show appropriate source tags
- System processes end-to-end from ingestion through narrative generation without manual intervention
